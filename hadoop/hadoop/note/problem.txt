随着企业要处理的数据量的不断增大，MapReduce的思想越来越受到重视，Hadoop是MapReduce的一个开源实现，
由于其良好的扩展性和容错性，已经得到了越来越广泛的应用，Hadoop作为一个开源的基础数据处理平台，虽然
其价值已经得得到了大家的认可，但是仍然存在很多问题：
(1)namenode，jobtracker的单点故障，Hadoop采用的是master/salver架构，该架构管理起来比较简单，但存在
致命的单点故障和空间容量不足等缺点，这已经严重影响到了Hadoop的可扩展性。
(2)HDFS小文件问题，在HDFS中，任何block，文件或者目录在内存中均以对象的形式存储，每个对象约占150byte
如果有1000,0000个小文件，每个文件占用一个block，则namenode需要2G空间，这样namenode需要2G空间，如果
存储1亿个文件，则namenode需要20G空间，这样纳闷哦的内存容量严重制约了集群的扩展。
(3)jobtracker同时进行监控和调度，负载过大，为了解决这个问题，yahoo已经开始设计了下一代Hadoop的MapReduce。
他们的主要设计思路是将监控和调度分离，独立出一个专门的组件进行监控，而jobtracker只负责总体调度，至于局
部调度交给作业所在的client。
(3)数据处理性能：
很多实验表明：其处理性能有很大的提升空间。Hadoop类似于数据库，需要有专门的优化工程师根据实际需要进行
优化，有人称为Hadoop Performance Optimaization 
	为了提高其数据性能，很多人开始优化Hadoop，当前主要有以下几个思路：
a.从应用程序的角度进行优化，由于MapReduce是逐行解析数据文件的，怎样在迭代的情况下，编写高效率的
应用程序是一种优化思路。
b.对Hadoop的参数进行优化，当前Hadoop中有190多个参数，怎样通过调整这些参数，让Hadoop系统运行的根快。也是
一种优化思路。
c.从系统的实现角度进行优化，这种优化的难度是最大的，它是从Hadoop的实现机制的角度，发现Hadoop的设计和实现
上的缺点，然后对源码进行修改，该方法难度比较大，但是效果比较明显。
2.从应用程序的角度进行优化：
a.避免不必要的Reduce任务，如果要处理的程序是已经分区的，或者对于一份数据，需要多次处理，可以先排序分区，然后
定义InputSpilt,将单个分区作为单个mapred的输入，在map中输入数据，将Reduce设为空。
b.外部数据的引入：
有些应用程序要使用外部文件，如字典，配置文件，这些文件需要在task之间共享，所以可以放到分布式缓存上去。
c.为job添加combiner可以大大减少shuffle阶段从maptask拷贝给远程reduce task的数据量，一般而言combine和Reduce
相同。

